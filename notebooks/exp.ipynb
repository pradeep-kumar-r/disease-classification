{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flag is false\n"
     ]
    }
   ],
   "source": [
    "def write_code(flag=True):\n",
    "    if flag:\n",
    "        code = \"\"\"print(\"Flag is true\")\n",
    "        \"\"\"\n",
    "    else:\n",
    "        code = \"\"\"print(\"Flag is false\")\n",
    "        \"\"\"\n",
    "        \n",
    "    return code\n",
    "\n",
    "exec(write_code(False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "def loaddata(dataset_path: Path):\n",
    "    ds=torch.load(dataset_path, weights_only=False)\n",
    "    dl=DataLoader(dataset=ds)\n",
    "    return dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Optional, Literal\n",
    "import os\n",
    "import pandas as pd\n",
    "from CNNClassifier.logger import logger\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: str,\n",
    "        images_path: str,\n",
    "        transform: Optional[transforms.Compose] = None,\n",
    "        type: Literal[\"train\", \"val\", \"test\"] = \"train\"\n",
    "    ):\n",
    "        self.data_path = data_path\n",
    "        self.images_path = images_path\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                              std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.type = type\n",
    "        self.data = []\n",
    "        self.classes = set()\n",
    "        data_df = pd.read_csv(data_path)\n",
    "        for _, row in data_df.iterrows():\n",
    "            image_name = row['images']\n",
    "            label = row['label']\n",
    "            self.data.append((image_name, label))\n",
    "            self.classes.add(label)\n",
    "        \n",
    "        self.classes = sorted(list(self.classes))\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "        self.idx_to_class = {v: k for k, v in self.class_to_idx.items()}\n",
    "        self.data = [(img_name, self.class_to_idx[label]) for img_name, label in self.data]\n",
    "        logger.info(f\"Created dataset with {len(self.data)} images and {len(self.classes)} classes\")\n",
    "    \n",
    "    def num_classes(self) -> int:\n",
    "        return len(self.classes)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
    "        if idx >= self.__len__():\n",
    "            logger.error(f\"Index {idx} is out of range\")\n",
    "            raise IndexError(f\"Index {idx} is out of range\")\n",
    "        \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        img_name, label = self.data[idx]\n",
    "        img_path = os.path.join(self.images_path, img_name)\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            image = self.transform(image)\n",
    "            return image, label\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading image {img_path}: {e}\")\n",
    "            raise e\n",
    "        \n",
    "    def export_dataset(self, save_path: Path) -> None:\n",
    "        try:\n",
    "            metadata = {\n",
    "                'data': self.data,  # List of (image_name, label) tuples\n",
    "                'data_path': self.data_path,\n",
    "                'images_path': self.images_path,\n",
    "                'transform': self.transform,\n",
    "                'class_to_idx': self.class_to_idx,\n",
    "                'type': self.type\n",
    "            }\n",
    "            \n",
    "            torch.save(metadata, save_path)\n",
    "            logger.info(f\"Dataset metadata exported to {save_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error exporting dataset metadata: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def __eq__(self, other: 'ImageDataset') -> bool:\n",
    "        return self.data == other.data and self.images_path == other.images_path and self.data_path == other.data_path and self.type == other.type and self.num_classes() == other.num_classes() and self.__len__() == other.__len__()\n",
    "     \n",
    "    @classmethod\n",
    "    def load_dataset(cls, metadata_path: Path) -> 'ImageDataset':\n",
    "        try:\n",
    "            metadata = torch.load(metadata_path, weights_only=False)\n",
    "            dataset = cls(\n",
    "                data_path=metadata['data_path'],\n",
    "                images_path=metadata['images_path'],\n",
    "                type=metadata['type']\n",
    "            )\n",
    "            dataset.data = metadata['data']\n",
    "            # dataset.class_to_idx = metadata['class_to_idx']\n",
    "            logger.info(f\"Load dataset from {metadata_path} with {len(dataset)} images and {dataset.num_classes()} classes\")\n",
    "            return dataset\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading dataset: {e}\")\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-19 01:23:40.499\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m42\u001b[0m - \u001b[1mCreated dataset with 5646 images and 4 classes\u001b[0m\n",
      "\u001b[32m2025-05-19 01:23:40.515\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m42\u001b[0m - \u001b[1mCreated dataset with 806 images and 4 classes\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_og = ImageDataset(\n",
    "    data_path=Path(\"../data/train/train_data.csv\"),\n",
    "    images_path=Path(\"../data/train/images\"),\n",
    "    type=\"train\"\n",
    ")\n",
    "\n",
    "val_og = ImageDataset(\n",
    "    data_path=Path(\"../data/val/val_data.csv\"),\n",
    "    images_path=Path(\"../data/val/images\"),\n",
    "    type=\"val\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-19 01:23:42.703\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mexport_dataset\u001b[0m:\u001b[36m81\u001b[0m - \u001b[1mDataset metadata exported to ../data/train/train_metadata.pt\u001b[0m\n",
      "\u001b[32m2025-05-19 01:23:42.709\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mexport_dataset\u001b[0m:\u001b[36m81\u001b[0m - \u001b[1mDataset metadata exported to ../data/val/val_metadata.pt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_og.export_dataset(Path(\"../data/train/train_metadata.pt\"))\n",
    "val_og.export_dataset(Path(\"../data/val/val_metadata.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-19 02:13:14.144\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m42\u001b[0m - \u001b[1mCreated dataset with 5646 images and 4 classes\u001b[0m\n",
      "\u001b[32m2025-05-19 02:13:14.145\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m100\u001b[0m - \u001b[1mLoad dataset from ../data/train/train_metadata.pt with 5646 images and 4 classes\u001b[0m\n",
      "\u001b[32m2025-05-19 02:13:14.160\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m42\u001b[0m - \u001b[1mCreated dataset with 806 images and 4 classes\u001b[0m\n",
      "\u001b[32m2025-05-19 02:13:14.160\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_dataset\u001b[0m:\u001b[36m100\u001b[0m - \u001b[1mLoad dataset from ../data/val/val_metadata.pt with 806 images and 4 classes\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_new = ImageDataset.load_dataset(Path(\"../data/train/train_metadata.pt\"))\n",
    "val_new = ImageDataset.load_dataset(Path(\"../data/val/val_metadata.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: True\n",
      "Val: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train: {train_og == train_new}\")\n",
    "print(f\"Val: {val_og == val_new}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
